{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0e3e81-7706-406b-a5f4-87edbaf551c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install langchain langchain-community langchainhub faiss-cpu python-dotenv\n",
    "\n",
    "# %pip install -U langchain-ollama\n",
    "\n",
    "# %pip install pymupdf\n",
    "\n",
    "# %pip install arxiv langchainhub requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6531a911-76c8-41a8-8f04-86f23d2c6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import arxiv\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc64ab75-a5fc-42b6-b455-edf07ef6a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 pages, split into 82 chunks.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"data/raw_papers/sample_paper.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages, split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3af8bd0-f343-43bd-b3a9-03d740c49684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "vectorstore.save_local(\"embeddings/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1cad17-dbcc-4e5f-9810-54f7d5f4b246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a926fff3-416c-43e3-bba9-766f20acad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know the answer to that question. The provided context appears to be a collection of papers and references related to protein generation and representation learning, but it does not explicitly state the main contribution of the paper \"SE(3) diffusion model with application to protein backbone generation\". To determine the main contribution, I would need more information about the specific paper being discussed.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main contribution of this paper?\"\n",
    "\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e682aa5a-4d4c-42ea-aa43-5bb3e8881a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Demystify Protein Generation with Hierarchical Conditional Diffusion Models\n",
      "PDF: http://arxiv.org/pdf/2507.18603v1\n",
      "Summary: Generating novel and functional protein sequences is critical to a wide range\n",
      "of applications in biology. Recent advancements in conditional diffusion models\n",
      "have shown impressive empirical performance in protein generation tasks.\n",
      "However, reliable generations of protein remain an open research question in de\n",
      "novo protein design, especially when it comes to conditional diffusion models.\n",
      "Considering the biological function of a protein is determined by multi-level\n",
      "structures, we propose a novel m...\n",
      "\n",
      "Title: Molecule Generation for Target Protein Binding with Hierarchical Consistency Diffusion Model\n",
      "PDF: http://arxiv.org/pdf/2503.00975v1\n",
      "Summary: Effective generation of molecular structures, or new chemical entities, that\n",
      "bind to target proteins is crucial for lead identification and optimization in\n",
      "drug discovery. Despite advancements in atom- and motif-wise deep learning\n",
      "models for 3D molecular generation, current methods often struggle with\n",
      "validity and reliability. To address these issues, we develop the Atom-Motif\n",
      "Consistency Diffusion Model (AMDiff), utilizing a joint-training paradigm for\n",
      "multi-view learning. This model features a...\n",
      "\n",
      "Title: Hierarchical protein backbone generation with latent and structure diffusion\n",
      "PDF: http://arxiv.org/pdf/2504.09374v1\n",
      "Summary: We propose a hierarchical protein backbone generative model that separates\n",
      "coarse and fine-grained details. Our approach called LSD consists of two\n",
      "stages: sampling latents which are decoded into a contact map then sampling\n",
      "atomic coordinates conditioned on the contact map. LSD allows new ways to\n",
      "control protein generation towards desirable properties while scaling to large\n",
      "datasets. In particular, the AlphaFold DataBase (AFDB) is appealing due as its\n",
      "diverse structure topologies but suffers fro...\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from arxiv import Client\n",
    "\n",
    "def fetch_paper_from_arxiv(query, max_results=3):\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        \n",
    "        query=query,\n",
    "        \n",
    "        max_results=max_results,\n",
    "        \n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "        \n",
    "    )\n",
    "\n",
    "    client = Client()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for result in client.results(search):\n",
    "        \n",
    "        paper_info = {\n",
    "            \n",
    "            \"title\": result.title,\n",
    "            \n",
    "            \"authors\": [a.name for a in result.authors],\n",
    "            \n",
    "            \"summary\": result.summary,\n",
    "            \n",
    "            \"url\": result.entry_id,\n",
    "            \n",
    "            \"pdf_url\": result.pdf_url\n",
    "            \n",
    "        }\n",
    "        \n",
    "        results.append(paper_info)\n",
    "\n",
    "    return results\n",
    "\n",
    "arxiv_results = fetch_paper_from_arxiv(\"Hierarchical conditional diffusion protein\")\n",
    "\n",
    "for res in arxiv_results:\n",
    "    \n",
    "    print(f\"\\nTitle: {res['title']}\\nPDF: {res['pdf_url']}\\nSummary: {res['summary'][:500]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c483cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'query': 'What is the main contribution of this paper?', 'result': \"I don't know the answer to that question based on the provided context. The text appears to be an abstract or summary of a conference paper, and it does not explicitly state what the main contribution of the paper is. It seems to provide some background information on protein design methods and representation learning for proteins, but it does not clearly outline the main findings or contributions of the paper.\"}\n"
     ]
    }
   ],
   "source": [
    "from utils.rag_agent import load_vectorstore, init_qa_chain, answer_query\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "\n",
    "vectorstore = load_vectorstore()\n",
    "\n",
    "qa_chain = init_qa_chain(vectorstore, model_name=\"llama3\")  \n",
    "\n",
    "query = \"What is the main contribution of this paper?\"\n",
    "\n",
    "response = answer_query(query, qa_chain)\n",
    "\n",
    "print(\"Answer:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950de7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF downloaded: data/raw_papers/2202.01319v1.pdf\n",
      "Loaded 35 pages and split into 64 chunks.\n",
      "FAISS index saved to: embeddings/faiss_index\n",
      "üîç Final Answer: {'query': \"From the entire content of this research paper, identify the main contribution. If it's not explicitly stated, infer it from the introduction, methodology, and conclusion.\", 'result': 'The main contribution of this research paper is not explicitly stated in the provided text. However, based on the introduction and methodology sections, I can infer that the main contribution is the application of deep learning techniques to traditional epidemiology, highlighting its potential for automating feature discovery and model fitting in data analysis.'}\n",
      "üìÑ Answering multiple research questions:\n",
      "\n",
      "‚û°Ô∏è Summarize the abstract of this paper.\n",
      "üß† {'query': 'Summarize the abstract of this paper.', 'result': \"I don't have enough information to summarize the abstract of this paper, as there is no abstract provided. The text appears to be a mix of explanations about Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), along with some examples from health research and background information on artificial intelligence (AI). If you could provide more context or clarify which paper you are referring to, I may be able to assist you further.\"}\n",
      "\n",
      "‚û°Ô∏è What is the main contribution of this paper?\n",
      "üß† {'query': 'What is the main contribution of this paper?', 'result': \"I don't know. The provided text does not appear to contain a summary or statement of the main contribution of the paper. It appears to be an introduction to deep learning in epidemiology, providing definitions and context for machine learning approaches.\"}\n",
      "\n",
      "‚û°Ô∏è What methodology is proposed in this research?\n",
      "üß† {'query': 'What methodology is proposed in this research?', 'result': 'According to the text, no specific methodology is explicitly proposed in this research. The text appears to be an overview or introduction to deep learning and its applications in epidemiology, rather than presenting a novel methodology. It discusses traditional approaches to data analysis in epidemiology and contrasts them with the use of deep learning algorithms, but it does not propose a new methodology or approach.'}\n",
      "\n",
      "‚û°Ô∏è What are the key findings or results?\n",
      "üß† {'query': 'What are the key findings or results?', 'result': \"I don't know. The text doesn't appear to present any specific key findings or results, but rather provides context and explanations about supervised learning and its application in epidemiology.\"}\n",
      "\n",
      "‚û°Ô∏è What are the conclusions or future work suggestions?\n",
      "üß† {'query': 'What are the conclusions or future work suggestions?', 'result': \"I don't know. The provided context does not contain any information about conclusions or future work suggestions. It appears to be an excerpt from a document discussing deep learning in health informatics, presenting various applications and challenges of using deep learning in this field. However, it does not mention specific conclusions or suggestions for future work.\"}\n"
     ]
    }
   ],
   "source": [
    "from utils.fetch_papers import fetch_paper_from_arxiv\n",
    "\n",
    "from utils.parse_pdf import extract_and_split_pdf\n",
    "\n",
    "from utils.chunk_embed import embed_chunks\n",
    "\n",
    "from utils.rag_agent import load_vectorstore, init_qa_chain, answer_query, ask_multiple_questions\n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "# ----------- USER INPUT -----------\n",
    "query_topic = \"fundamental concepts in machine learning and deep learning\"\n",
    "\n",
    "user_question = \"From the entire content of this research paper, identify the main contribution. If it's not explicitly stated, infer it from the introduction, methodology, and conclusion.\"\n",
    "\n",
    "# ----------- STEP 1: Fetch Arxiv paper metadata -----------\n",
    "arxiv_results = fetch_paper_from_arxiv(query_topic, max_results=1)\n",
    "\n",
    "paper = arxiv_results[0]\n",
    "\n",
    "pdf_url = paper['pdf_url']\n",
    "\n",
    "pdf_filename = f\"{pdf_url.split('/')[-1]}.pdf\"\n",
    "\n",
    "pdf_path = f\"data/raw_papers/{pdf_filename}\"\n",
    "\n",
    "# ----------- STEP 2: Download the PDF -----------\n",
    "if not os.path.exists(pdf_path):\n",
    "    \n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        \n",
    "        f.write(response.content)\n",
    "        \n",
    "    print(f\"‚úÖ PDF downloaded: {pdf_path}\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(f\"üìÑ PDF already exists: {pdf_path}\")\n",
    "\n",
    "# ----------- STEP 3: Extract text & split into chunks -----------\n",
    "chunks = extract_and_split_pdf(pdf_path)\n",
    "\n",
    "# ----------- STEP 4: Embed and save vectorstore -----------\n",
    "embed_chunks(chunks)\n",
    "\n",
    "# ----------- STEP 5: Load vectorstore and create QA chain -----------\n",
    "vectorstore = load_vectorstore()\n",
    "\n",
    "qa_chain = init_qa_chain(vectorstore, model_name=\"llama3\")\n",
    "\n",
    "# ----------- STEP 6: Ask main question -----------\n",
    "response = answer_query(user_question, qa_chain)\n",
    "\n",
    "print(\"üîç Final Answer:\", response)\n",
    "\n",
    "# ----------- STEP 7: Ask multiple insightful questions -----------\n",
    "ask_multiple_questions(qa_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c3e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index already exists. Skipping embedding.\n",
      "üìÑ You can now ask anything about the research papers in raw_papers/\n",
      "\n",
      "üìå Answer (based strictly on the PDF content):\n",
      " {'query': 'explain me in detail about the fundamental concepts in machine learning and deep learning', 'result': \"Based on the provided context, I'll try to summarize the fundamental concepts in machine learning and deep learning:\\n\\n**Machine Learning Fundamentals**\\n\\n1. **Supervised Learning**: In this approach, you have labeled data (input-output pairs) that allows an algorithm to learn patterns and make predictions.\\n2. **Unsupervised Learning**: Without labeled data, algorithms discover hidden patterns or groupings within the data.\\n3. **Reinforcement Learning**: This involves learning through trial-and-error by receiving rewards or penalties for certain actions.\\n\\n**Machine Learning Algorithms**\\n\\n1. **Supervised Learning Algorithms**: Focus of this article (unsaid but implied).\\n2. **Unsupervised Learning Algorithms**: Clustering, dimensionality reduction, density estimation.\\n3. **Reinforcement Learning Algorithms**: Trial-and-error learning, action-value learning.\\n\\n**Deep Learning Fundamentals**\\n\\n1. **Neural Networks**: Inspired by brain physiology, artificial neural networks consist of interconnected nodes (neurons) that process information.\\n2. **Processing Layers**: Each layer consists of neurons that receive inputs from previous layers and produce outputs to feed into subsequent layers.\\n3. **Gradient Descent**: A commonly used optimization algorithm for training deep learning models, which adjusts parameter values based on calculated gradients.\\n\\n**Common Deep Learning Architectures**\\n\\n1. **Fully-Connected Neural Networks (FNNs)**: The most fundamental type of deep learning networks, where each neuron receives inputs from all neurons in the previous layer.\\n2. **Feed-Forward Neural Networks**: A subcategory of FNNs with one-way flow of information.\\n\\n**Machine Learning and Deep Learning Terminology**\\n\\n1. **Bias Term**: An offset or constant term added to the model's output.\\n2. **Model Intercept**: The value that the model returns when all input features are zero.\\n3. **Training**: The process of adjusting a model's parameters based on labeled data.\\n4. **Model Fitting**: Synonymous with training.\\n5. **Derivation Set**: A set of data used to evaluate a model's performance.\\n6. **Bagging**: Bootstrap aggregation, which combines multiple models or predictions for improved performance.\\n7. **L1 and L2 Regularization**: Techniques that modify the learning process by adding penalties to the cost function to prevent overfitting.\\n8. **Recall**, **Sensitivity**, **Precision**, **Positive Predictive Value**: Metrics used to evaluate a model's performance.\\n\\nThis summary should provide a solid foundation for understanding fundamental concepts in machine learning and deep learning.\"}\n"
     ]
    }
   ],
   "source": [
    "from utils.parse_pdf import extract_and_split_pdf\n",
    "\n",
    "from utils.chunk_embed import embed_chunks\n",
    "\n",
    "from utils.rag_agent import load_vectorstore, init_qa_chain, answer_query\n",
    "\n",
    "import os\n",
    "\n",
    "pdf_folder = \"data/raw_papers\"\n",
    "\n",
    "index_path = \"embeddings/faiss_index\"\n",
    "\n",
    "# =============== STEP 1: Extract + Embed if index doesn't exist ===============\n",
    "if not os.path.exists(index_path):\n",
    "    \n",
    "    print(\"üì¶ No FAISS index found ‚Äî embedding all PDFs now...\")\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    all_chunks = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        \n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        \n",
    "        print(f\"üîç Processing: {pdf_path}\")\n",
    "        \n",
    "        chunks = extract_and_split_pdf(pdf_path)\n",
    "        \n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    embed_chunks(all_chunks)\n",
    "    \n",
    "    print(\"‚úÖ All PDFs embedded and FAISS index created.\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"‚úÖ FAISS index already exists. Skipping embedding.\")\n",
    "\n",
    "# =============== STEP 2: Load Vectorstore + LLM Chain ===============\n",
    "vectorstore = load_vectorstore()\n",
    "\n",
    "qa_chain = init_qa_chain(vectorstore, model_name=\"llama3\")\n",
    "\n",
    "# =============== STEP 3: User Query from Research Papers ===============\n",
    "print(\"üìÑ You can now ask anything about the research papers in raw_papers/\")\n",
    "\n",
    "user_query = input(\"üß† Enter your question:\\n‚Üí \")\n",
    "\n",
    "answer = answer_query(user_query, qa_chain)\n",
    "\n",
    "print(\"\\nüìå Answer (based strictly on the PDF content):\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5cd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
