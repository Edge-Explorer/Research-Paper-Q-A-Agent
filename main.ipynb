{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0e3e81-7706-406b-a5f4-87edbaf551c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install langchain langchain-community langchainhub faiss-cpu python-dotenv\n",
    "\n",
    "# %pip install -U langchain-ollama\n",
    "\n",
    "# %pip install pymupdf\n",
    "\n",
    "# %pip install arxiv langchainhub requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6531a911-76c8-41a8-8f04-86f23d2c6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import arxiv\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc64ab75-a5fc-42b6-b455-edf07ef6a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 pages, split into 82 chunks.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"data/raw_papers/sample_paper.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages, split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3af8bd0-f343-43bd-b3a9-03d740c49684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "vectorstore.save_local(\"embeddings/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1cad17-dbcc-4e5f-9810-54f7d5f4b246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a926fff3-416c-43e3-bba9-766f20acad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know the answer to that question. The text does not explicitly state what the main contribution of the paper is. It appears to be a conference proceeding about a paper titled \"SE(3) diffusion model with application to protein backbone generation\", but it does not provide a clear summary or highlight the main contribution of the paper.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main contribution of this paper?\"\n",
    "\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e682aa5a-4d4c-42ea-aa43-5bb3e8881a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Demystify Protein Generation with Hierarchical Conditional Diffusion Models\n",
      "PDF: http://arxiv.org/pdf/2507.18603v1\n",
      "Summary: Generating novel and functional protein sequences is critical to a wide range\n",
      "of applications in biology. Recent advancements in conditional diffusion models\n",
      "have shown impressive empirical performance in protein generation tasks.\n",
      "However, reliable generations of protein remain an open research question in de\n",
      "novo protein design, especially when it comes to conditional diffusion models.\n",
      "Considering the biological function of a protein is determined by multi-level\n",
      "structures, we propose a novel m...\n",
      "\n",
      "Title: Molecule Generation for Target Protein Binding with Hierarchical Consistency Diffusion Model\n",
      "PDF: http://arxiv.org/pdf/2503.00975v1\n",
      "Summary: Effective generation of molecular structures, or new chemical entities, that\n",
      "bind to target proteins is crucial for lead identification and optimization in\n",
      "drug discovery. Despite advancements in atom- and motif-wise deep learning\n",
      "models for 3D molecular generation, current methods often struggle with\n",
      "validity and reliability. To address these issues, we develop the Atom-Motif\n",
      "Consistency Diffusion Model (AMDiff), utilizing a joint-training paradigm for\n",
      "multi-view learning. This model features a...\n",
      "\n",
      "Title: Hierarchical protein backbone generation with latent and structure diffusion\n",
      "PDF: http://arxiv.org/pdf/2504.09374v1\n",
      "Summary: We propose a hierarchical protein backbone generative model that separates\n",
      "coarse and fine-grained details. Our approach called LSD consists of two\n",
      "stages: sampling latents which are decoded into a contact map then sampling\n",
      "atomic coordinates conditioned on the contact map. LSD allows new ways to\n",
      "control protein generation towards desirable properties while scaling to large\n",
      "datasets. In particular, the AlphaFold DataBase (AFDB) is appealing due as its\n",
      "diverse structure topologies but suffers fro...\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from arxiv import Client\n",
    "\n",
    "def fetch_paper_from_arxiv(query, max_results=3):\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        \n",
    "        query=query,\n",
    "        \n",
    "        max_results=max_results,\n",
    "        \n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "        \n",
    "    )\n",
    "\n",
    "    client = Client()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for result in client.results(search):\n",
    "        \n",
    "        paper_info = {\n",
    "            \n",
    "            \"title\": result.title,\n",
    "            \n",
    "            \"authors\": [a.name for a in result.authors],\n",
    "            \n",
    "            \"summary\": result.summary,\n",
    "            \n",
    "            \"url\": result.entry_id,\n",
    "            \n",
    "            \"pdf_url\": result.pdf_url\n",
    "            \n",
    "        }\n",
    "        \n",
    "        results.append(paper_info)\n",
    "\n",
    "    return results\n",
    "\n",
    "arxiv_results = fetch_paper_from_arxiv(\"Hierarchical conditional diffusion protein\")\n",
    "\n",
    "for res in arxiv_results:\n",
    "    \n",
    "    print(f\"\\nTitle: {res['title']}\\nPDF: {res['pdf_url']}\\nSummary: {res['summary'][:500]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c483cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\Research_Agent\\utils\\rag_agent.py:25: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know. The text provides an abstract and citations to related papers, but it does not explicitly state what the main contribution of the paper is.\n"
     ]
    }
   ],
   "source": [
    "from utils.rag_agent import load_vectorstore, init_qa_chain, answer_query\n",
    "\n",
    "vectorstore = load_vectorstore()\n",
    "\n",
    "qa_chain = init_qa_chain(vectorstore, model_name=\"llama3\")\n",
    "\n",
    "query = \"What is the main contribution of this paper?\"\n",
    "\n",
    "response = answer_query(query, qa_chain)\n",
    "\n",
    "print(\"Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d950de7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_rag_query' from 'utils.rag_agent' (c:\\Users\\ASUS\\OneDrive\\Desktop\\Research_Agent\\utils\\rag_agent.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse_pdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_and_split_pdf\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunk_embed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m embed_chunks\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrag_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_rag_query\n\u001b[32m      9\u001b[39m query_topic = \u001b[33m\"\u001b[39m\u001b[33mPyTorch graph transformer\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# ← user sets this\u001b[39;00m\n\u001b[32m     11\u001b[39m user_question = \u001b[33m\"\u001b[39m\u001b[33mWhat is the main contribution of this paper?\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# ← can change to anything\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'run_rag_query' from 'utils.rag_agent' (c:\\Users\\ASUS\\OneDrive\\Desktop\\Research_Agent\\utils\\rag_agent.py)"
     ]
    }
   ],
   "source": [
    "from utils.fetch_papers import fetch_paper_from_arxiv\n",
    "\n",
    "from utils.parse_pdf import extract_and_split_pdf\n",
    "\n",
    "from utils.chunk_embed import embed_chunks\n",
    "\n",
    "from utils.rag_agent import run_rag_query\n",
    "\n",
    "query_topic = \"PyTorch graph transformer\"  # ← user sets this\n",
    "\n",
    "user_question = \"What is the main contribution of this paper?\"  # ← can change to anything\n",
    "\n",
    "papers = fetch_paper_from_arxiv(query_topic, max_results=1)\n",
    "\n",
    "if not papers:\n",
    "    \n",
    "    print(\"No results found.\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    top_paper = papers[0]\n",
    "    \n",
    "    print(f\"\\n Title: {top_paper['title']}\\n PDF: {top_paper['pdf_url']}\")\n",
    "\n",
    "    import requests, os\n",
    "    \n",
    "    pdf_url = top_paper[\"pdf_url\"]\n",
    "    \n",
    "    local_path = f\"data/raw_papers/auto_paper.pdf\"\n",
    "    \n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    with open(local_path, \"wb\") as f:\n",
    "        \n",
    "        f.write(response.content)\n",
    "\n",
    "    chunks = extract_and_split_pdf(local_path)\n",
    "    \n",
    "    retriever = embed_chunks(chunks)\n",
    "\n",
    "    print(\"\\n Question:\", user_question)\n",
    "    \n",
    "    result = run_rag_query(user_question, retriever)\n",
    "    \n",
    "    print(\"\\n Answer:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3e448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
